{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Modelle\n",
    "\n",
    "In dieser Übung werden wir die linearen Modelle, die in den Vorlesungsfolien vorkommen, selbst implementieren.\n",
    "Wir starten mit den Iris-Daten, die Sie schon aus der Vorlesung kennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
       "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
       "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
       "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
       "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
       "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
       "5            5.4         3.9          1.7         0.4     Iris-setosa\n",
       "6            4.6         3.4          1.4         0.3     Iris-setosa\n",
       "7            5.0         3.4          1.5         0.2     Iris-setosa\n",
       "8            4.4         2.9          1.4         0.2     Iris-setosa\n",
       "9            4.9         3.1          1.5         0.1     Iris-setosa\n",
       "10           5.4         3.7          1.5         0.2     Iris-setosa\n",
       "11           4.8         3.4          1.6         0.2     Iris-setosa\n",
       "12           4.8         3.0          1.4         0.1     Iris-setosa\n",
       "13           4.3         3.0          1.1         0.1     Iris-setosa\n",
       "14           5.8         4.0          1.2         0.2     Iris-setosa\n",
       "15           5.7         4.4          1.5         0.4     Iris-setosa\n",
       "16           5.4         3.9          1.3         0.4     Iris-setosa\n",
       "17           5.1         3.5          1.4         0.3     Iris-setosa\n",
       "18           5.7         3.8          1.7         0.3     Iris-setosa\n",
       "19           5.1         3.8          1.5         0.3     Iris-setosa\n",
       "20           5.4         3.4          1.7         0.2     Iris-setosa\n",
       "21           5.1         3.7          1.5         0.4     Iris-setosa\n",
       "22           4.6         3.6          1.0         0.2     Iris-setosa\n",
       "23           5.1         3.3          1.7         0.5     Iris-setosa\n",
       "24           4.8         3.4          1.9         0.2     Iris-setosa\n",
       "25           5.0         3.0          1.6         0.2     Iris-setosa\n",
       "26           5.0         3.4          1.6         0.4     Iris-setosa\n",
       "27           5.2         3.5          1.5         0.2     Iris-setosa\n",
       "28           5.2         3.4          1.4         0.2     Iris-setosa\n",
       "29           4.7         3.2          1.6         0.2     Iris-setosa\n",
       "..           ...         ...          ...         ...             ...\n",
       "120          6.9         3.2          5.7         2.3  Iris-virginica\n",
       "121          5.6         2.8          4.9         2.0  Iris-virginica\n",
       "122          7.7         2.8          6.7         2.0  Iris-virginica\n",
       "123          6.3         2.7          4.9         1.8  Iris-virginica\n",
       "124          6.7         3.3          5.7         2.1  Iris-virginica\n",
       "125          7.2         3.2          6.0         1.8  Iris-virginica\n",
       "126          6.2         2.8          4.8         1.8  Iris-virginica\n",
       "127          6.1         3.0          4.9         1.8  Iris-virginica\n",
       "128          6.4         2.8          5.6         2.1  Iris-virginica\n",
       "129          7.2         3.0          5.8         1.6  Iris-virginica\n",
       "130          7.4         2.8          6.1         1.9  Iris-virginica\n",
       "131          7.9         3.8          6.4         2.0  Iris-virginica\n",
       "132          6.4         2.8          5.6         2.2  Iris-virginica\n",
       "133          6.3         2.8          5.1         1.5  Iris-virginica\n",
       "134          6.1         2.6          5.6         1.4  Iris-virginica\n",
       "135          7.7         3.0          6.1         2.3  Iris-virginica\n",
       "136          6.3         3.4          5.6         2.4  Iris-virginica\n",
       "137          6.4         3.1          5.5         1.8  Iris-virginica\n",
       "138          6.0         3.0          4.8         1.8  Iris-virginica\n",
       "139          6.9         3.1          5.4         2.1  Iris-virginica\n",
       "140          6.7         3.1          5.6         2.4  Iris-virginica\n",
       "141          6.9         3.1          5.1         2.3  Iris-virginica\n",
       "142          5.8         2.7          5.1         1.9  Iris-virginica\n",
       "143          6.8         3.2          5.9         2.3  Iris-virginica\n",
       "144          6.7         3.3          5.7         2.5  Iris-virginica\n",
       "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
       "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
       "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
       "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
       "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie in der Vorlesung können wir die Daten plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cac78a3978>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot iris \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfs = df.loc[df.Name != \"Iris-setosa\",:]\n",
    "#print(dfs)\n",
    "\n",
    "plt.plot(dfs.loc[:,\"SepalWidth\"],dfs.loc[:,\"PetalLength\"],'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst schauen wir uns das einfache lineare Modell an, das den Zusammenhang zwischen PetalLength und SepalWidth darstellt. Wir werden für das Fitten von linearen Modellen das Python-Paket statsmodels verwenden (`https://www.statsmodels.org`). Die Spezifikation von linearen Modellen funktioniert hier ganz ähnlich wie in den R-Beispielen, die in der Vorlesung vorkommen.\n",
    "\n",
    "Um die `ols` Formeln besser zu verstehen, lohnt sich ein Blick hier: (https://patsy.readthedocs.io/en/latest/formulas.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>PetalLength</td>   <th>  R-squared:         </th> <td>   0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   36.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Dec 2021</td> <th>  Prob (F-statistic):</th> <td>2.99e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:04:22</td>     <th>  Log-Likelihood:    </th> <td> -106.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   217.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   222.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>    1.2021</td> <td>    0.619</td> <td>    1.942</td> <td> 0.055</td> <td>   -0.026</td> <td>    2.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SepalWidth</th> <td>    1.2897</td> <td>    0.214</td> <td>    6.023</td> <td> 0.000</td> <td>    0.865</td> <td>    1.715</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.767</td> <th>  Durbin-Watson:     </th> <td>   0.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.034</td> <th>  Jarque-Bera (JB):  </th> <td>   6.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.575</td> <th>  Prob(JB):          </th> <td>  0.0446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.414</td> <th>  Cond. No.          </th> <td>    28.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            PetalLength   R-squared:                       0.270\n",
       "Model:                            OLS   Adj. R-squared:                  0.263\n",
       "Method:                 Least Squares   F-statistic:                     36.28\n",
       "Date:                Tue, 14 Dec 2021   Prob (F-statistic):           2.99e-08\n",
       "Time:                        13:04:22   Log-Likelihood:                -106.48\n",
       "No. Observations:                 100   AIC:                             217.0\n",
       "Df Residuals:                      98   BIC:                             222.2\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.2021      0.619      1.942      0.055      -0.026       2.430\n",
       "SepalWidth     1.2897      0.214      6.023      0.000       0.865       1.715\n",
       "==============================================================================\n",
       "Omnibus:                        6.767   Durbin-Watson:                   0.895\n",
       "Prob(Omnibus):                  0.034   Jarque-Bera (JB):                6.222\n",
       "Skew:                           0.575   Prob(JB):                       0.0446\n",
       "Kurtosis:                       3.414   Cond. No.                         28.2\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "results = smf.ols('PetalLength ~ SepalWidth', data=dfs).fit()\n",
    "#?smf.ols\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aufgabe*: Interpretieren Sie die Ergebnisse! Was bedeuten die Koeffizienten, $R^2$, p-Werte?\n",
    "\n",
    "Intercept = Wert, an dem Regressions-Linie die y-Achse schneidet (expected mean value of Y when all X=0)\n",
    "\n",
    "$R^2$ Werte: percentage variation in dependent that is explained by independent variables \n",
    "\n",
    "p-Wert: Signifikanz der Regression\n",
    "\n",
    "(https://jyotiyadav99111.medium.com/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisierung\n",
    "\n",
    "Betrachten Sie nun den stark korrelierten Datensatz von Seite 23 des Foliensatzes `Linear Models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1caca2305c0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"corr_data.csv\")\n",
    "#print(df)\n",
    "plt.plot(df.loc[:,\"x1\"],df.loc[:,\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aufgabe*: Fitten Sie ein lineares Modell mit y als abhängiger Variablen und x1 und x2 als unabhängigen Variablen. Interpretieren Sie erneut die Ergebnisse. Was ist das Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.993</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7531.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Dec 2021</td> <th>  Prob (F-statistic):</th> <td>5.19e-108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:04:22</td>     <th>  Log-Likelihood:    </th> <td> -136.69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   279.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   287.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    4.9804</td> <td>    0.095</td> <td>   52.277</td> <td> 0.000</td> <td>    4.791</td> <td>    5.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>   -5.8432</td> <td>    7.006</td> <td>   -0.834</td> <td> 0.406</td> <td>  -19.746</td> <td>    8.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    7.8352</td> <td>    7.006</td> <td>    1.118</td> <td> 0.266</td> <td>   -6.069</td> <td>   21.739</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.274</td> <th>  Durbin-Watson:     </th> <td>   1.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.529</td> <th>  Jarque-Bera (JB):  </th> <td>   0.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.015</td> <th>  Prob(JB):          </th> <td>   0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.426</td> <th>  Cond. No.          </th> <td>    864.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.993\n",
       "Method:                 Least Squares   F-statistic:                     7531.\n",
       "Date:                Tue, 14 Dec 2021   Prob (F-statistic):          5.19e-108\n",
       "Time:                        13:04:22   Log-Likelihood:                -136.69\n",
       "No. Observations:                 101   AIC:                             279.4\n",
       "Df Residuals:                      98   BIC:                             287.2\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      4.9804      0.095     52.277      0.000       4.791       5.169\n",
       "x1            -5.8432      7.006     -0.834      0.406     -19.746       8.059\n",
       "x2             7.8352      7.006      1.118      0.266      -6.069      21.739\n",
       "==============================================================================\n",
       "Omnibus:                        1.274   Durbin-Watson:                   1.815\n",
       "Prob(Omnibus):                  0.529   Jarque-Bera (JB):                0.769\n",
       "Skew:                          -0.015   Prob(JB):                        0.681\n",
       "Kurtosis:                       3.426   Cond. No.                         864.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.ols('y ~ x1 + x2', data=df).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Werte der Koeffizienten sehr hoch, gesamter P-Wert sehr gering, Standardabweichung der Koeffizienten sehr hoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aufgabe*: Verwenden Sie Ridge Regression, um ein lineares Modell zu fitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    4.922549\n",
       "x1           1.963074\n",
       "x2           0.027962\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.ols('y ~ x1 + x2', data=df)\n",
    "results = model.fit_regularized(L1_wt=0.1,alpha=0.01)\n",
    "results.params #parameters for intercept, x1, x2: what we expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistische Regression mit sklearn\n",
    "\n",
    "Im Machine Learning sind wir oft gar nicht so sehr an den Parametern des Modells interessiert, sondern daran, dass das Modell gute Vorhersagen liefert. Für solche Zwecke ist die logistische Regression aus dem Paket `sklearn` besser geeignet -- dort werden Dinge, die in statsmodels recht mühsam sind, wie das Encoding der Klasse, der Umgang mit mehr als zwei Klassen usw. direkt erledigt. Schauen Sie sich dafür die folgende Dokumentation an: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Teilen Sie den Iris-Datensatz in zwei Teilmengen auf: Eine Trainingsmenge, die 80% der Daten enthält, und eine Testmenge, die die restlichen 20% der Daten enthält. Trainieren Sie dann ein Logit-Modell mit den Trainingsdaten und erzeugen Sie eine Vorhersage für die Klasse der Testdaten. Welcher Anteil der Testdaten wurde korrekt klassifiziert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) \n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "yhat = clf.predict(X_test)\n",
    "np.mean(yhat == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
